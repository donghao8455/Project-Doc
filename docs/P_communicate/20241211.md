## 2024/12/11 第五次交流

我感觉这个模型的有些模块是值得借鉴的，特别是语义特征编码器提取模态的语义特征，将高层次的语义和上下文关系进行提取（因为我感觉对于我们的数据集如果使用专用的编码器去进行特征的提取，可能提取不了非常有用的特征，这些特征对后续的对齐可能没有很大的影响），还有`Cross-CPC`模块，对于没有完美匹配对齐的模态，使用了自回归模型总结了历史信息，模型可以通过对比学习在细粒度级别上预测其他模态下一步可能的信息

`CMG`模型总览：

![image-20241130134048602](..\assets\image-20241130134048602.png)

> 在预训练阶段，通过跨模态泛化`CMG`，将不同模态映射到一个统一的离散空间中，使离散的潜在代码具有相同语义的不同模态之间共享
>
> 模型将多模态序列映射到一个共同的离散语义空间中，其中的多模态序列信息可能是不受约束的（也就是说模态间不是完美对齐的），文中使用`VGGSound24k`数据集训练视觉-听觉-文本的统一表示学习，经过训练，得到一个统一离散的潜在空间
>
> - 使用语义编码器提取模态的语义特征（语义信息是指数据中包含的高层抽象的概念或语义含义（包含了高层次语义和上下文关系等等），而不是低层次的视觉特征（如边缘、纹理和颜色等等）），再使用特定编码器提取模态的剩余特征
>
>   使用`DCID`模块提取细粒度的语义信息，并且将其与每个模态中相应的模态特定信息分开，双跨模式信息解缠
>
>   语义编码器提取模态的语义特征：结合了通道注意力机制（通过全局平均池化和线性变换生成通道注意力权重）和自注意力机制（通过全局平均特征和局部特征的交互，生成空间注意力权重）
>
> - 使用向量运算将语义特征映射到细粒度级别的离散表示
>
>   使用`VQ-VAE`模块将语义特征压缩为离散变量，确保压缩后的离散变量通过重构损失仍保持原始的语义信息
>
>   代码中定义了一个名为 `Cross_VQEmbeddingEMA_AVT` 的函数，用于对音频、视频和文本的语义特征进行向量量化`（Vector Quantization, VQ）`，并计算交叉模态的损失。该模块结合了指数移动平均`（Exponential Moving Average, EMA）`（使用指数移动平均`EMA`来代替`VQ`损失，因为`EMA`更加稳健，重建损失可以保证压缩后的潜在代码仍然保持不同模态的语义信息）和交叉模态一致性损失，以实现跨模态的特征对齐和量化
>
> - 在codebook中共享多个模态的信息
>
>   在理想情况下，对于相同语义的不同模态语义特征编码应该映射到相同的离散潜在代码中，但是如果没有有效的监督，模态差异将导致不同模态的语义特征会聚到codebook中的不同区域，因此，需要使用一定方式进行缓解：
>
>   采用互信息MI估计，衡量两个随机变量之间的依赖关系
>
>   - 互信息MI最大化：学习捕捉输入数据中有意义的和有用的数据表示，从而提高各种下游任务性能，不同模态中模态不可知语义特征之间的MI最大化
>
>     对比预测编码`CPC`采用回归模型和对比估计来捕获长期关系，同时保持序列中的局部特性，从而提高序列中的互信息最大化，`Cross-CPC`将`CPC`拓展到跨模态收缩预测，对于没有完美匹配对齐的模态，使用了自回归模型总结了历史信息，模型可以通过对比学习在细粒度级别上预测其他模态下一步可能的信息，文章中使用两个单层的`LSTM`来总结两个不同模态当前时刻之前的语义特征，得到两个模态的上下文表示，，然后使用一个模态的上下文表示预测另一个模态k个未来时间步
>
>   - 互信息MI最小化：减少两个随机变量之间的依赖关系，同时保留相关信息，多用于解缠表示学习，每个模态中不可知语义特征和模态特定特征的MI最小化（减少每种模态中语义信息和模态特定信息之间的相关性）
>
>     对比对数比上限`CLUB`将MI估计于对比学习相结合，以近似MI上限，`CLUB`可以有效的优化MI上界，在信息解缠方面表现优势，使用`CLUB`优化语义特征和模态特定特征
>
> - 最后将细粒度级别的模态信息与剩余特征组合回一起，重建原始特征
>
> 将预训练好的多模态编码器应用于下游 的任务，包括：跨模态事件分类（音频和视频中都有一个主要事件，使用单一的模态训练事件分类器，并直接评估另一个模态在当前分类器上的性能）、跨模态事件定位（部分数据和音频有精细的事件标签进行注释，在一种模态上进行事件定位，然后直接将模态转移到另一个模态进行测试，使用准确率评估性能）、基于查询的视频分割和跨域和模态事件定位，同时在执行下游任务期间，预训练编码器的参数是被冻结的
>
> 当只有一个模态A有注释信息时，模型可以根据预训练时获得的共享离散空间，将A模态中学到的知识转移到其他模态，实现零镜头泛化能力
>
> `CMG`模型方法的核心是区分和细化跨模态数据中的共享语义内容，同时消除特定于每种模态的冗余信息。这种设计使模型能够学习更精细和通用的跨模态表示，即使在模态之间没有完美对齐的情况下，也能够提取公共语义。因此，这种方法增强了模型在更复杂任务中的性能。

`vggsound-avel40k.csv`文件的内容：根据`split`字段将数据划分为训练集、测试集和验证集

![image-20241211204349337](..\assets\image-20241211204349337.png)

具体使用的video和audio数据保存形式使用的是`.pkl`文件进行序列化保存的

`vggsoundCategories2Prompts.csv`：该文件包括了类别标签到文本提示的映射（对于类别事件给出了一段稍微具体的文本描述）

![image-20241211211750304](..\assets\image-20241211211750304.png)

**我觉得`CMG`模型中是有许多知识值得去学习和研究的，如：使用语义编码器去提取语义特征（语义特征有着高层抽象的概念或语义含义），将语义特征进行离散化处理，通过codebook将不同模态的相同语义特征映射到同一个区域，对于没有完全对齐的模态，使用`Cross-CPC`跨模态收缩预测等等**

今年该论文的作者又在这篇论文的基础上提出了一篇新的论文：`Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment`

`CMG`模型使用了双跨模态信息去纠缠（`DCID`）模型利用统一码本的方式，实现细粒度表示和跨模态泛化方面表现出了良好的效果，然而，该算法仍然受到所有通道的同等对待和对小事件信息的忽视的阻碍，导致不相关通道的干扰和在细粒度任务中的性能受限（限制1：在嵌入空间利用效率和对齐粒度方面存在局限性；限制2：可能会忽略特定模态特有的关键事件信息，从而限制模型在细粒度任务中的性能），于是作者又提出了一种不需训练的码本最佳化（`TOC`）方法，借由在统一空间中选择重要通道而不需重新训练，以提升模型效能。此外，还引入了分层双跨模态信息去纠缠（`H-DCID`）方法，将信息分离和对齐扩展到两个层次，捕获更多的跨模态细节

对于`CMG`模型在在嵌入空间利用效率和对齐粒度方面存在局限性，作者引入了码本（`TOC`）的免训练优化机制来缓解这个问题，`TOC`通过计算优化码本，不需要额外的训练参数，从而增强在下游任务中计算和选择预训练模型的码本内的特征的能力。这种方法实现了无训练效果的改进，减少了额外的训练成本，并降低了下游任务的训练参数要求。

对于可能会忽略特定模态特有的关键事件信息，从而限制模型在细粒度任务中的性能这一个限制，作者提出了`H-DCID`模型（分层双交叉模态信息解纠缠），保留了`DCID`在实现统一多模态表达方面的优势，同时通过分层对齐机制显着增强了模型在处理细粒度信息方面的灵活性和精度，引入了二次对齐过程来有效地提取和对齐这些信息。

这两种方式的改进在有些复杂的下游任务中是可以有较好的效果的，但是对于下游任务`AVE`的跨模态事件分类中效果是不如`CMG`模型优秀的，所以说对于具体的下游任务对模型进行具体的选择是有必要的

![image-20241213140119950](..\assets\image-20241213140119950.png)

***

交流反馈：

- 继续深入的研究一下这个模型中具体模块的代码，深入了解其原理
- 也可以去了解一下`prototypical network`这个知识点