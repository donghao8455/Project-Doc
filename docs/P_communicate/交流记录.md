# 交流记录

## 2024/10/31  第一次交流

吴老师，我最近看了一些关于CLIP的内容，结合着想了一下镁化炉数据多模态对齐：

我觉得对齐多模态数据主要考虑的是，为了可以精确的得到这个电流数据的预测提前区

我认为对齐需要拿出一个异常周期的视频数据和时序电流数据（电流的数据可以往前拿多一点，因为电流相较于视频会提前产生异常）（也不能拿多个异常周期，因为在后续对齐时，可能前一个电流的异常数据和后一个视频的异常数据计算的相似度也会比较高，这个情况是需要避免的）

难点：像传统的CLIP，使用的是图片模态和文本模态，对应的文本模态间内容的差异往往是比较大的（比如输入的文本内容是：这是汽车、这是飞机），这样与图片模态特征数据进行的相似度计算的结果往往可以较好的体现差距（正样本和负样本是比较好区分的），但是如果使用镁化炉的数据，这个的序列数据的特征差异往往是比较小的（异常程度当作具体的差异？异常、未异常、小异常、大异常？），我认为计算相似度差异的区分度可能不会有这么明显的效果（可能最后得到的矩阵各个位置的相似度值差距不大）

问题：

1. 电流数据的异常特征是如何体现的？（没有看过具体的数据，不是很清楚）
2. 你给我的思路图中的`ev`、`ec`中的每个单元是不是表示一个时刻的视频数据模态特征和时序电流数据模态特征？

***

交流反馈：

- 电流的异常会比较反常，波动很大的时候也很可能是正常的，这个很难去通过经验来判断，所以还是需要通过视觉

- 确实像你说的，需要把电流提前取比较多的值，同时也避免取过多。比如一般的异常是电流3-5分钟后视频会产生异常，那么我们取的电流信息就需要覆盖5分钟之前即可；

- 异常不分大小，只是binary的分类（二分类）问题；

- 图中的`ev` `ec`指的是特征，输入包含了时序信息



## 2024/11/09  第二次交流

吴老师，我这段时间主要了解了一下你发我的数据（拿这些数据尝试去进行简单的特征提取，试了一些transformer相关的特征提取代码）和一些关于对比学习相关的内容

- 对于对比学习，训练一个像CLIP这样的大模型是不太可能的（该模型训练所需的数据量是很大），但是我觉得CLIP这个模型的思路是可以借鉴的，通过余弦相似度来计算两个模态特征的相关性

- 模型初步设计想法：在数据集中找出一段比较有代表性的连续数据，考虑50个时间步长的视频模态数据，100个时间步长的时序电流模态数据（后50个时间步长的数据与视频模态的数据保持时间上的对齐），时序电流模态上做一个大小为50的滑动窗口，视频模态上做一个大小为50的固定窗口，分别提取窗口中两个模态的特征，计算模态间的余弦相似度，随着滑动窗口的不断滑动，找到两个模态相似度最高的位置（相似度矩阵的对角线上都是正样本即可，因为是二分类的问题，除了对角线的其他位置还是会有正样本的），从而得到时序电流的提前时间。

  但是上面这个方法细想有点问题：在提取特征的时候，需不需要将异常/正常标签label的数据信息放到各个模态中一起进行特征的提取，但是这样时序电流数据模态是没有准确的异常/正常标签的（感觉数据集里面的label只是标注了视频模态数据的异常/正常信息，对于时序电流数据没有额外的标注说明，那么后面如果使用对比学习进行训练的时候，是不是可以理解为当三相电流在短时间内有较大的上升时，就认为发生了异常），如果需要融入label到各个模态中，是要通过怎么个方式进行融入呢？还是说不需要label的信息加入到特征中，只要通过视频模态的特征来对齐时序电流的模态即可？（是不是可以认为电流忽然较大幅度的上升和视频中`rgb`颜色变成红色/黄色/白色，这两个特征认为是相似度比较高）

  还有一个就是数据中的一个时间步长（两次间隔的采样）具体是多少时间？

个人思路：

![image-20241122144203595](..\assets\image-20241122144203595.png)

***

交流反馈：

个人提出的思路是可以的，但是要重点关注怎么从特征层面上进行多模态的对齐，多去了解一下`cross-modal alignment`或者`multi-modal alignment`的文章



## 2024/11/22 第三次交流

吴老师，我这段时间主要看了一些跨模态对齐相关的文章，梳理了一些有代表性的对比模型和问题，主要涉及到模态间的对齐方法、特征间的模态融合和对不同数据模态的特征提取

可借鉴的对比模型：

- **模态间的对齐**，在`BLIP-2`大模型中，使用了`Q-Former`（`Query Transformer`）来弥补模态间的差距，实现了特征的对齐，具体而言就是一个模态提供Q，另一个模态提供K和V，对齐实现方式是通过一个模态的K和V去查找另一个模态的Q（这种对齐方式是一个全局上的模态对齐，但是忽略了模态间的时间特性）

- `AlignVSR`模型用来对齐视频模态的数据和音频模态的数据，采用了全局的对齐和局部的对齐方式：

  ![image-20241121110434214](..\assets\image-20241121110434214.png)

  - 全局对齐（类似于`Q-Former`的方式）：对准的实现从每个视频帧到音频单元组的交叉注意力过程，即视频特征是查询（Q），而音频单元作用键（K）和值（V），这种对齐是全局的对齐，视频可以与任何音频单元进行对齐（为了更好地保留视频的时间信息，在视频特征中必须涉及位置编码）
  - 局部对齐：全局对齐不利用音频和视频模态之间的时间对齐，因此设计了局部对齐机制来解决这个问题，图中u表示音频单元经量化的数目，v表示视频特征序列，a表示音频特征序列（由于采样频率不同，一个视频帧对应三个音频帧），T表示是视频帧的总数

- 对于**模态间特征的融合**，在一篇情感分析的文章里面，提出了一种基于`Transformer-Encoder`的多层融合模型，利用多方向自注意的思想实现了文本和图像的标记级特征的对齐和融合，并利用`MLF`进行特征融合，从而提高了模型的抽象能力

  ![image-20241119160843178](..\assets\image-20241119160843178-1732265824593-1.png)

  `MLF`多层融合模块，使用文本（`BERT`）-图像（`ResNet`）编码器来获得文本和图像的隐藏表示（提取特征），之后需要将图像特征的维度转化为与文本特征相同的维度，将图像特征输入到基于多层`Transformer-Encoder`的图像`Transformer`层，得到图像序列特征的最终编码

  对于特征的融合，将文本的特征与图像序列特征连接起来，再使用一个多层变换器编码器作为文本图像融合层，对齐和融合多模态特征，得到文本和图像的融合序列特征

  ![image-20241120100248113](..\assets\image-20241120100248113.png)

  在获得了文本和图像融合的序列特征后，序列特征不能用于分类任务，因此，我们使用一个简单的注意力层来获得多模态表示

  这个模型中还提出了基于标签的对比学习（`LBCL`）和基于数据的对比学习任务（`DBCL`），其中基于标签的对比学习我认为是可以进行借鉴的：

  文章中的基于标签的对比学习，是根据情感标签将每个批次中的数据分为正面和负面示例，对于多模态数据的负标签，批次中具有相同负标签的数据作为正例（粉红色的正方形），而没有负标签的数据作为负例（灰色的正方形），该算法主要包括两个步骤：第一步是根据批量中的数据标签生成去掩码标签；第二步是计算损失矩阵，利用去掩码标签和损失矩阵得到最终的损失

  **结合基于标签的对比学习，我觉得可以把标签的思想使用到镁化炉中，镁化炉中有label数据，标记了正常和异常状态：正常状态（0）和异常状态（1）通过进行特征融合`MLF`多层融合模型，时序电流的特征为T，视频模态的特征为I，最后计算标记对比学习损失**

- **对于不同模态数据的特征提取**，`OneLLM`大模型是一种使用统一框架将八种模态与语言相结合的`MLLM`，通过一个统一的多模式编码器和一个渐进的多模式对齐管道来实现这一点

  ![image-20241122181748879](..\assets\image-20241122181748879.png)

  上图为`OneLLM`架构，该架构由模态标记化器、通用编码器、通用投影模块（`UPM`）和`LLM`组成，其中：

  - 模态标记器是`2D/1D`卷积层，用于将输入信号变换成标记序列

  - **通用编码器`Universal Encoder`**是用于提取高维特征的冻结视觉语言模型

    使用`CLIPViT`作为通用计算引擎，对于视频信号，将所有视频帧并行送入编码器，并在帧之间执行令牌式平均以加快训练，对于标记拼接，可以进一步增强模型的视频理解能力

  - `UPM`由几个投影专家和模态路由器组成，以将输入信号与语言对齐。对于对齐阶段，训练模态标记器和`UPM`，并保持`LLM`冻结

问题：

- 对于多模态对齐方面的文章，感觉很多文章中都是基于大模型来实现的（基于这些大模型做下游任务），这些预训练好的大模型涉及到的数据集都是非常庞大的，而且算力的要求也是非常大的

  我觉得基于镁化炉数据集做出来的对齐模型是不是通用性没有这么好？（做出来的对比模型可能只是适用这个特定的数据集，感觉如果使用镁化炉的数据来进行对齐好像有点过于单薄，但是如果加一点公开的数据进行对比实验，类似的视频-时序电流模态的公开数据集不太好找，如果使用图片-文本的数据集来进行后续设计的模型的对比实验，感觉有点不搭，思路有点卡壳，希望老师可以指点一下）

- 对比学习要实现效果好，需要满足两个条件：1、负例要尽可能的多；2、负例要尽可能一致，我们的数据集应该如何有效的进行构建负例？什么样的模态特征可以算作一个正例？（视频区域的`rgb`颜色变黄/白和三相电流忽然大幅度升高/三相电流的三相数据差异较大）

***

交流反馈：

- 继续去了解情感分析那篇文章中的研究方法

- 不用考虑通用性，基于镁化炉的数据集做这个工业背景下的专项研究

- 采用特征拉远拉近的方式进行对比学习
- 数据集中的电流异常表现机理是：电流数据变化较小的说明发生了异常，电流数据变化较大的表示镁化炉正在工作（正常工作的情况下，电流变化是比较大的）这一点与之前的理解是有偏差的



## 2024/11/30 第四次交流

吴老师，我这个星期主要去复现和看了一下之前那篇情感分析论文中的代码，有一些体会和问题：

1. 对于那篇情感分析论文中的代码深入的了解了一下，我发现，它用到的基于标签的对比学习是一个自监督的对比学习，也就是说对于数据，是没有提前人工打标签标注的，是在训练的过程中以自监督的形式生成的情感标签
2. 从代码和思路图中看出，作者的重心主要是在跨模态融合和基于数据增强的对比学习方面，对于基于标签的对比学习基本是简单带过，我感觉基于标签的对比学习更像是一个模型最后的优化实验（加入了这个模块，稍微提升了一点性能指标）
3. 感觉基于自监督的对比学习不太适合镁化炉这个工业场景，而且原论文中的实际模态（图片和文本模态）和我们的数据模态差别挺大的，于是我沿着基于监督的对比学习和基于时序数据的对比学习去找相关论文

我找到了这么一篇文章：

`Achieving Cross Modal Generalization with Multimodal Unified Representation`

这篇文章提出了一种新的跨模态综合方法，即跨模态综合（跨模态泛化）（`CMG`），它解决了在预训练过程中从成对的多模态数据中学习统一的离散表示的问题，然后在下游任务中，**当只标注一个模态时，该模型在其他模态中也能达到`zero-shot`泛化能力**

文章的重点在于研究如何实现细粒度层面上的多模态序列统一表达，提出了多模态 EMA，利用 teacher-student 机制，让不同模态相互指导，在离散空间中互相靠近，并最终将具有相同语义的不同模态变量收敛到一起

![image-20241130134048602](..\assets\image-20241130134048602.png)

通过预训练获得了一个多模态通用的 codebook，以及能将这些模态映射到该离散空间的对应模态的编码器，在下游任务中，利用预训练得到的编码器和 codebook，只对一种模态进行训练，然后让将训练得到的模型零样本的迁移到其他未知模态中进行测试，测试的任务包括跨模态事件分类，跨模态事件定位，跨模态视频分类和跨模态检索

模态的具体方法：

1. 给定两个成对的模态数据![image-20241130182122898](..\assets\image-20241130182122898.png)，使用两个语义编码器去提取模态的无关特征，使用两个特定模态数据提取编码器去提取模态的剩余特征：

   ![image-20241130182401296](..\assets\image-20241130182401296.png)

2. 对语义特征使用 `VQ-VAE `在细粒度层面上对模态信息进行离散化，使用了同一个 `codebook`，然后将离散化后的向量与模态无关向量合并后，重构回原来的特征，在本文中，使用指数平均移动（`EMA`）来代替 `VQ loss`，重构` loss` 则保证了压缩后的离散变量仍旧具有不同模态的语义信息，但是如果在没有监督的情况下，会出现不同模态的语义特征会收敛到 `codebook` 中的不同区域，难以实现下游的各种跨模态泛化任务，于是作者又提出了以下的模块来缓解这个问题：

   1. 对偶跨模态信息解耦（`DCID`）

      首先是在每个模态内部将` modal-agnostic semantic features` 与 `modal-specific features` 的互信息最小化（`CLUB`）（可以减少每种模态中语义信息和模态特定信息之间的相关性），其次是对不同模态中 `modal-agnostic semantic features`的互信息最大化（`Cross-CPC`）（`CPC`表示对比学习预测，被广泛的用于序列特征的自监督训练中，对于人类来说，我们可以根据当前模态中的已知序列去预测另一个模态中可能发生的情况，因此，本文提出了`Cross-CPC`，可以通过这种细粒度的跨模态对比预测将不同模态中相关的语义信息互相提取出来，有效地最大化不同模态之间的细粒度互信息）

   2. `Multi-modal Exponential Moving Average`

      提出了一个多模态 EMA 方法，能够让不同模态在离散化过程中，互相作为 `teacher-student`迭代更新，最终收敛到一起，促进模态间的对齐

我觉得这篇文章的立足点是比较符合我们的需求，**对于多模态的数据，从标记模态获得的知识转移到下游任务中的其他未知模态，使模型能够有效地泛化，促进基于已知模态的跨模态知识转移**

***

交流反馈：

- 这篇文章的思路是可以的，我们主要学习的是一个模态对齐的表征
- 这篇文章的写作思路与我们的实际问题是比较符合的，很多内容可以照着我们电熔镁炉的背景修改一下即可
- 给了电熔镁炉的背景知识，用于开题报告
- 给了多模态电熔镁炉异常检测的代码，用于参考



## 2024/12/11 第五次交流

我感觉这个模型的有些模块是值得借鉴的，特别是语义特征编码器提取模态的语义特征，将高层次的语义和上下文关系进行提取（因为我感觉对于我们的数据集如果使用专用的编码器去进行特征的提取，可能提取不了非常有用的特征，这些特征对后续的对齐可能没有很大的影响），还有`Cross-CPC`模块，对于没有完美匹配对齐的模态，使用了自回归模型总结了历史信息，模型可以通过对比学习在细粒度级别上预测其他模态下一步可能的信息

`CMG`模型总览：

![image-20241130134048602](..\assets\image-20241130134048602.png)

> 在预训练阶段，通过跨模态泛化`CMG`，将不同模态映射到一个统一的离散空间中，使离散的潜在代码具有相同语义的不同模态之间共享
>
> 模型将多模态序列映射到一个共同的离散语义空间中，其中的多模态序列信息可能是不受约束的（也就是说模态间不是完美对齐的），文中使用`VGGSound24k`数据集训练视觉-听觉-文本的统一表示学习，经过训练，得到一个统一离散的潜在空间
>
> - 使用语义编码器提取模态的语义特征（语义信息是指数据中包含的高层抽象的概念或语义含义（包含了高层次语义和上下文关系等等），而不是低层次的视觉特征（如边缘、纹理和颜色等等）），再使用特定编码器提取模态的剩余特征
>
>   使用`DCID`模块提取细粒度的语义信息，并且将其与每个模态中相应的模态特定信息分开，双跨模式信息解缠
>
>   语义编码器提取模态的语义特征：结合了通道注意力机制（通过全局平均池化和线性变换生成通道注意力权重）和自注意力机制（通过全局平均特征和局部特征的交互，生成空间注意力权重）
>
> - 使用向量运算将语义特征映射到细粒度级别的离散表示
>
>   使用`VQ-VAE`模块将语义特征压缩为离散变量，确保压缩后的离散变量通过重构损失仍保持原始的语义信息
>
>   代码中定义了一个名为 `Cross_VQEmbeddingEMA_AVT` 的函数，用于对音频、视频和文本的语义特征进行向量量化`（Vector Quantization, VQ）`，并计算交叉模态的损失。该模块结合了指数移动平均`（Exponential Moving Average, EMA）`（使用指数移动平均`EMA`来代替`VQ`损失，因为`EMA`更加稳健，重建损失可以保证压缩后的潜在代码仍然保持不同模态的语义信息）和交叉模态一致性损失，以实现跨模态的特征对齐和量化
>
> - 在codebook中共享多个模态的信息
>
>   在理想情况下，对于相同语义的不同模态语义特征编码应该映射到相同的离散潜在代码中，但是如果没有有效的监督，模态差异将导致不同模态的语义特征会聚到codebook中的不同区域，因此，需要使用一定方式进行缓解：
>
>   采用互信息MI估计，衡量两个随机变量之间的依赖关系
>
>   - 互信息MI最大化：学习捕捉输入数据中有意义的和有用的数据表示，从而提高各种下游任务性能，不同模态中模态不可知语义特征之间的MI最大化
>
>     对比预测编码`CPC`采用回归模型和对比估计来捕获长期关系，同时保持序列中的局部特性，从而提高序列中的互信息最大化，`Cross-CPC`将`CPC`拓展到跨模态收缩预测，对于没有完美匹配对齐的模态，使用了自回归模型总结了历史信息，模型可以通过对比学习在细粒度级别上预测其他模态下一步可能的信息，文章中使用两个单层的`LSTM`来总结两个不同模态当前时刻之前的语义特征，得到两个模态的上下文表示，，然后使用一个模态的上下文表示预测另一个模态k个未来时间步
>
>   - 互信息MI最小化：减少两个随机变量之间的依赖关系，同时保留相关信息，多用于解缠表示学习，每个模态中不可知语义特征和模态特定特征的MI最小化（减少每种模态中语义信息和模态特定信息之间的相关性）
>
>     对比对数比上限`CLUB`将MI估计于对比学习相结合，以近似MI上限，`CLUB`可以有效的优化MI上界，在信息解缠方面表现优势，使用`CLUB`优化语义特征和模态特定特征
>
> - 最后将细粒度级别的模态信息与剩余特征组合回一起，重建原始特征
>
> 将预训练好的多模态编码器应用于下游 的任务，包括：跨模态事件分类（音频和视频中都有一个主要事件，使用单一的模态训练事件分类器，并直接评估另一个模态在当前分类器上的性能）、跨模态事件定位（部分数据和音频有精细的事件标签进行注释，在一种模态上进行事件定位，然后直接将模态转移到另一个模态进行测试，使用准确率评估性能）、基于查询的视频分割和跨域和模态事件定位，同时在执行下游任务期间，预训练编码器的参数是被冻结的
>
> 当只有一个模态A有注释信息时，模型可以根据预训练时获得的共享离散空间，将A模态中学到的知识转移到其他模态，实现零镜头泛化能力
>
> `CMG`模型方法的核心是区分和细化跨模态数据中的共享语义内容，同时消除特定于每种模态的冗余信息。这种设计使模型能够学习更精细和通用的跨模态表示，即使在模态之间没有完美对齐的情况下，也能够提取公共语义。因此，这种方法增强了模型在更复杂任务中的性能。

`vggsound-avel40k.csv`文件的内容：根据`split`字段将数据划分为训练集、测试集和验证集

![image-20241211204349337](..\assets\image-20241211204349337.png)

具体使用的video和audio数据保存形式使用的是`.pkl`文件进行序列化保存的

`vggsoundCategories2Prompts.csv`：该文件包括了类别标签到文本提示的映射（对于类别事件给出了一段稍微具体的文本描述）

![image-20241211211750304](..\assets\image-20241211211750304.png)

**我觉得`CMG`模型中是有许多知识值得去学习和研究的，如：使用语义编码器去提取语义特征（语义特征有着高层抽象的概念或语义含义），将语义特征进行离散化处理，通过codebook将不同模态的相同语义特征映射到同一个区域，对于没有完全对齐的模态，使用`Cross-CPC`跨模态收缩预测等等**

今年该论文的作者又在这篇论文的基础上提出了一篇新的论文：`Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment`

`CMG`模型使用了双跨模态信息去纠缠（`DCID`）模型利用统一码本的方式，实现细粒度表示和跨模态泛化方面表现出了良好的效果，然而，该算法仍然受到所有通道的同等对待和对小事件信息的忽视的阻碍，导致不相关通道的干扰和在细粒度任务中的性能受限（限制1：在嵌入空间利用效率和对齐粒度方面存在局限性；限制2：可能会忽略特定模态特有的关键事件信息，从而限制模型在细粒度任务中的性能），于是作者又提出了一种不需训练的码本最佳化（`TOC`）方法，借由在统一空间中选择重要通道而不需重新训练，以提升模型效能。此外，还引入了分层双跨模态信息去纠缠（`H-DCID`）方法，将信息分离和对齐扩展到两个层次，捕获更多的跨模态细节

对于`CMG`模型在在嵌入空间利用效率和对齐粒度方面存在局限性，作者引入了码本（`TOC`）的免训练优化机制来缓解这个问题，`TOC`通过计算优化码本，不需要额外的训练参数，从而增强在下游任务中计算和选择预训练模型的码本内的特征的能力。这种方法实现了无训练效果的改进，减少了额外的训练成本，并降低了下游任务的训练参数要求。

对于可能会忽略特定模态特有的关键事件信息，从而限制模型在细粒度任务中的性能这一个限制，作者提出了`H-DCID`模型（分层双交叉模态信息解纠缠），保留了`DCID`在实现统一多模态表达方面的优势，同时通过分层对齐机制显着增强了模型在处理细粒度信息方面的灵活性和精度，引入了二次对齐过程来有效地提取和对齐这些信息。

这两种方式的改进在有些复杂的下游任务中是可以有较好的效果的，但是对于下游任务`AVE`的跨模态事件分类中效果是不如`CMG`模型优秀的，所以说对于具体的下游任务对模型进行具体的选择是有必要的

![image-20241213140119950](..\assets\image-20241213140119950.png)

***

交流反馈：

- 继续深入的研究一下这个模型中具体模块的代码，深入了解其原理
- 也可以去了解一下`prototypical network`这个知识点